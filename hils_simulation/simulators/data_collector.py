"""
DataCollectorSimulator - HILSç”¨ãƒ‡ãƒ¼ã‚¿åé›†å™¨

ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ä¸­ã®å…¨ãƒ‡ãƒ¼ã‚¿ã‚’åé›†ã—ã€HDF5å½¢å¼ã§ä¿å­˜ã™ã‚‹ã€‚

åé›†ãƒ‡ãƒ¼ã‚¿:
- Controller: command, error, position, velocity
- Bridge(cmd): stats
- Plant: measured_thrust, status
- Bridge(sense): stats
- Env: position, velocity, acceleration, force
"""

import json
from datetime import datetime
from pathlib import Path

import mosaik_api

try:
    import h5py
except ImportError:
    h5py = None

meta = {
    "type": "time-based",
    "models": {
        "Collector": {
            "public": True,
            "params": ["output_dir", "minimal_mode"],
            "attrs": [],  # ä»»æ„ã®å±æ€§ã‚’å—ã‘å…¥ã‚Œã‚‹ï¼ˆå‹•çš„åé›†ï¼‰
            "any_inputs": True,  # ä»»æ„ã®å…¥åŠ›ã‚’å—ã‘å…¥ã‚Œã‚‹
        },
    },
}


class DataCollectorSimulator(mosaik_api.Simulator):
    """
    ãƒ‡ãƒ¼ã‚¿åé›†å™¨ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚¿ãƒ¼ï¼ˆHILSç‰ˆï¼‰

    å…¨ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚¿ãƒ¼ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’åé›†ã—ã€HDF5ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ã€‚
    """

    def __init__(self):
        super().__init__(meta)
        self.entities = {}
        self.step_size = 1  # 1msæ¯ã«è¨˜éŒ²
        self.data_log = []
        self.time_resolution = 0.001
        self.step_ms = self.time_resolution * 1000
        self.all_keys = set()  # ã‚­ãƒ¼ã‚’åŠ¹ç‡çš„ã«è¿½è·¡

    def init(
        self,
        sid,
        time_resolution=0.001,
        step_size=1,
    ):
        """
        åˆæœŸåŒ–

        Args:
            sid: ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚¿ãƒ¼ID
            time_resolution: æ™‚é–“è§£åƒåº¦ï¼ˆ0.001 = 1msï¼‰
            step_size: ã‚¹ãƒ†ãƒƒãƒ—ã‚µã‚¤ã‚º
        """
        self.sid = sid
        self.time_resolution = time_resolution
        self.step_size = step_size
        self.step_ms = self.time_resolution * 1000 if self.time_resolution else 1.0
        return self.meta

    def create(self, num, model, output_dir=None, minimal_mode=False):
        """
        ãƒ‡ãƒ¼ã‚¿ã‚³ãƒ¬ã‚¯ã‚¿ãƒ¼ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã®ä½œæˆ

        Args:
            num: ä½œæˆæ•°
            model: ãƒ¢ãƒ‡ãƒ«å
            output_dir: å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
            minimal_mode: æœ€å°é™ã®ãƒ‡ãƒ¼ã‚¿ã®ã¿è¨˜éŒ²ï¼ˆtime, position, velocityï¼‰
        """
        entities = []

        for i in range(num):
            eid = f"{model}_{i}"
            target_dir = Path(output_dir) if output_dir else Path.cwd()
            target_dir.mkdir(parents=True, exist_ok=True)

            self.entities[eid] = {
                "current_time": 0,
                "output_dir": target_dir,
                "minimal_mode": minimal_mode,
            }

            entities.append({"eid": eid, "type": model})
            mode_str = " (minimal mode)" if minimal_mode else ""
            print(f"[DataCollector] Created {eid} -> {target_dir}{mode_str}")

        return entities

    def step(self, time, inputs, max_advance=None):
        """
        ãƒ‡ãƒ¼ã‚¿åé›†ã‚¹ãƒ†ãƒƒãƒ—

        Args:
            time: ç¾åœ¨æ™‚åˆ» [ms]
            inputs: å…¥åŠ›ãƒ‡ãƒ¼ã‚¿
        """
        # å®Ÿæ™‚é–“ [ç§’]
        real_time = time * self.time_resolution
        time_ms = time * self.step_ms

        for (
            eid,
            entity,
        ) in self.entities.items():
            entity["current_time"] = time

            if eid in inputs:
                # Check if minimal mode is enabled
                minimal_mode = entity.get("minimal_mode", False)

                # ãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ³ãƒˆã®ä½œæˆï¼ˆç”Ÿãƒ‡ãƒ¼ã‚¿ã®ã¾ã¾ä¿å­˜ã€JSONå¤‰æ›ã¯æœ€å¾Œã«å®Ÿè¡Œï¼‰
                data_point = {
                    "time_ms": time_ms,  # ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³æ™‚åˆ» [ms]
                    "time_s": real_time,  # å®Ÿæ™‚é–“ [s]
                }

                # å…¨å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã‚’åé›†ï¼ˆæœ€å°é™ã®å‡¦ç†ï¼‰
                for attr, values in inputs[eid].items():
                    # Minimal mode: only collect position and velocity from Env
                    if minimal_mode:
                        if attr not in ["position", "velocity"]:
                            continue

                    for source_eid, value in values.items():
                        # Minimal mode: only Env data
                        if minimal_mode and "Env" not in source_eid:
                            continue

                        # å±æ€§åã¨ã‚½ãƒ¼ã‚¹IDã§ã‚­ãƒ¼ã‚’ä½œæˆ
                        key = f"{attr}_{source_eid}"

                        # ã‚­ãƒ¼ã‚’è¨˜éŒ²
                        self.all_keys.add(key)

                        # ç”Ÿãƒ‡ãƒ¼ã‚¿ã‚’ãã®ã¾ã¾ä¿å­˜
                        data_point[key] = value

                        # è¾æ›¸å‹ã®å ´åˆã€å„è¦ç´ ã‚‚è¨˜éŒ²ï¼ˆãƒ—ãƒ­ãƒƒãƒˆç”¨ï¼‰
                        # Minimal mode: skip dict expansion to save time
                        if isinstance(value, dict) and not minimal_mode:
                            for k, v in value.items():
                                subkey = f"{key}_{k}"
                                data_point[subkey] = v
                                self.all_keys.add(subkey)

                # data_logã«ã®ã¿ä¿å­˜ï¼ˆé‡è¤‡ã‚’é¿ã‘ã‚‹ï¼‰
                self.data_log.append(data_point)

        return time + self.step_size

    def get_data(self, outputs):
        """
        ãƒ‡ãƒ¼ã‚¿å–å¾—ï¼ˆé€šå¸¸ã¯ä½¿ç”¨ã—ãªã„ï¼‰
        """
        return {}

    def finalize(self):
        """
        ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³çµ‚äº†æ™‚ã®å‡¦ç†

        åé›†ã—ãŸå…¨ãƒ‡ãƒ¼ã‚¿ã‚’HDF5ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ã€‚
        """
        if not self.entities:
            print("[DataCollector] No entities to finalize.")
            return

        if h5py is None:
            print("[DataCollector] âš ï¸  h5py not available; skipped HDF5 export.")
            return

        if not self.data_log:
            print("[DataCollector] No data collected; nothing to write.")
            return

        # å‡ºåŠ›å…ˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        first_entity = next(iter(self.entities.values()))
        output_dir: Path = first_entity["output_dir"]
        output_dir.mkdir(parents=True, exist_ok=True)

        output_path = output_dir / "hils_data.h5"

        print(f"\n[DataCollector] ğŸ’¾ Saving {len(self.data_log)} data points to HDF5...")

        # å…¨ã‚­ãƒ¼ã‚’åé›†ï¼ˆã‚¹ãƒ†ãƒƒãƒ—ä¸­ã«è¿½è·¡ã—ãŸã‚­ãƒ¼ã‚’ä½¿ç”¨ï¼‰
        self.all_keys.add("time_ms")
        self.all_keys.add("time_s")
        all_keys = sorted(self.all_keys)

        with h5py.File(output_path, "w") as h5_file:
            # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
            h5_file.attrs["created_at"] = datetime.utcnow().isoformat() + "Z"
            h5_file.attrs["num_samples"] = len(self.data_log)
            h5_file.attrs["time_resolution"] = self.time_resolution

            # æ™‚åˆ»ãƒ‡ãƒ¼ã‚¿ã¯å…±é€šã‚°ãƒ«ãƒ¼ãƒ—ã«
            time_group = h5_file.create_group("time")

            # ãƒãƒ¼ãƒ‰ã”ã¨ã«ã‚°ãƒ«ãƒ¼ãƒ—ã‚’ä½œæˆ
            node_groups = {}

            # å„å±æ€§ã‚’ãƒãƒ¼ãƒ‰ã”ã¨ã«åˆ†é¡
            import re

            for key in all_keys:
                if key in ["time_ms", "time_s"]:
                    # æ™‚åˆ»ãƒ‡ãƒ¼ã‚¿ã¯ç‰¹åˆ¥æ‰±ã„
                    target_group = time_group
                    dataset_name = key
                else:
                    # ã‚­ãƒ¼ã‚’ãƒ‘ãƒ¼ã‚¹: attr_SimName-ID.EntityID[_suffix]
                    # ä¾‹1: buffer_size_BridgeSim-0.CommBridge_0 -> node=BridgeSim-0.CommBridge_0, attr=buffer_size
                    # ä¾‹2: compensated_output_InverseCompSim-0.cmd_compensator -> node=InverseCompSim-0.cmd_compensator, attr=compensated_output
                    # ä¾‹3: command_ControllerSim-0.PIDController_0_thrust -> node=ControllerSim-0.PIDController_0, attr=command_thrust

                    # Match: attr_prefix + SimName-ID. + rest
                    sim_match = re.match(r"([a-z_]+)_([A-Z][a-zA-Z]*Sim-\d+)\.(.+)", key)

                    if sim_match:
                        attr_prefix = sim_match.group(1)
                        sim_name = sim_match.group(2)
                        rest = sim_match.group(3)

                        # Parse 'rest' to separate entity_id from sub-attribute suffixes
                        # Known sub-attribute suffixes (from dict expansion)
                        known_suffixes = ["thrust", "duration"]

                        parts = rest.split("_")
                        suffix_idx = None

                        # Find if any known suffix exists
                        for i, part in enumerate(parts):
                            if part in known_suffixes:
                                suffix_idx = i
                                break

                        if suffix_idx is not None:
                            # Entity ID is everything before the suffix
                            entity_id = "_".join(parts[:suffix_idx])
                            attr_suffix = "_".join(parts[suffix_idx:])
                            attr_name = f"{attr_prefix}_{attr_suffix}"
                        else:
                            # No known suffix - entire rest is entity_id
                            entity_id = rest
                            attr_name = attr_prefix

                        node_name = f"{sim_name}.{entity_id}"

                        # ãƒãƒ¼ãƒ‰ã‚°ãƒ«ãƒ¼ãƒ—ã‚’ä½œæˆï¼ˆåˆå›ã®ã¿ï¼‰
                        if node_name not in node_groups:
                            # ã‚°ãƒ«ãƒ¼ãƒ—åã«ä½¿ãˆãªã„æ–‡å­—ã‚’ç½®æ›
                            safe_node_name = node_name.replace(".", "_")
                            node_groups[node_name] = h5_file.create_group(safe_node_name)

                        target_group = node_groups[node_name]
                        dataset_name = attr_name
                    else:
                        # ãƒ‘ãƒ¼ã‚¹ã§ããªã„å ´åˆã¯ãƒ«ãƒ¼ãƒˆã«é…ç½®
                        target_group = h5_file
                        dataset_name = key

                # ãƒ‡ãƒ¼ã‚¿åé›†
                column = []
                for entry in self.data_log:
                    value = entry.get(key)

                    if value is None:
                        column.append(float("nan"))
                    elif isinstance(value, dict):
                        # è¾æ›¸å‹ã¯JSONæ–‡å­—åˆ—ã«å¤‰æ›ï¼ˆfinalizeæ™‚ã®ã¿ï¼‰
                        column.append(json.dumps(value))
                    elif isinstance(value, str):
                        # æ–‡å­—åˆ—ã¯ãã®ã¾ã¾
                        column.append(value)
                    elif isinstance(value, (int, float)):
                        column.append(float(value))
                    else:
                        column.append(str(value))

                # ãƒ‡ãƒ¼ã‚¿å‹ã«å¿œã˜ã¦ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½œæˆ
                if column and isinstance(column[0], str):
                    # æ–‡å­—åˆ—ãƒ‡ãƒ¼ã‚¿
                    target_group.create_dataset(
                        name=dataset_name,
                        data=column,
                        dtype=h5py.string_dtype(),
                    )
                else:
                    # æ•°å€¤ãƒ‡ãƒ¼ã‚¿
                    target_group.create_dataset(
                        name=dataset_name,
                        data=column,
                    )

            # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä¸€è¦§ã‚’è¡¨ç¤ºï¼ˆãƒãƒ¼ãƒ‰ã”ã¨ï¼‰
            print("[DataCollector] âœ… Saved datasets by node:")

            # æ™‚åˆ»ãƒ‡ãƒ¼ã‚¿
            print("\n  [time/]")
            for key in sorted(time_group.keys()):
                dataset = time_group[key]
                print(f"    - {key}: {dataset.shape} {dataset.dtype}")

            # ãƒãƒ¼ãƒ‰ãƒ‡ãƒ¼ã‚¿
            for node_name in sorted(node_groups.keys()):
                node_group = node_groups[node_name]
                print(f"\n  [{node_name}/]")
                for key in sorted(node_group.keys()):
                    dataset = node_group[key]
                    print(f"    - {key}: {dataset.shape} {dataset.dtype}")

        print(f"[DataCollector] ğŸ“ Output: {output_path}")


if __name__ == "__main__":
    mosaik_api.start_simulator(DataCollectorSimulator())
