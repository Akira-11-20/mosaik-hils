"""
DataCollectorSimulator - HILSç”¨ãƒ‡ãƒ¼ã‚¿åé›†å™¨

ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ä¸­ã®å…¨ãƒ‡ãƒ¼ã‚¿ã‚’åé›†ã—ã€HDF5å½¢å¼ã§ä¿å­˜ã™ã‚‹ã€‚

åé›†ãƒ‡ãƒ¼ã‚¿:
- Controller: command, error, position, velocity
- Bridge(cmd): stats
- Plant: measured_thrust, status
- Bridge(sense): stats
- Env: position, velocity, acceleration, force
"""
import json
from datetime import datetime
from pathlib import Path

import mosaik_api
import numpy as np

try:
    import h5py
except ImportError:
    h5py = None

meta = {
    "type": "time-based",
    "models": {
        "Collector": {
            "public": True,
            "params": ["output_dir"],
            "attrs": [],  # ä»»æ„ã®å±æ€§ã‚’å—ã‘å…¥ã‚Œã‚‹ï¼ˆå‹•çš„åé›†ï¼‰
            "any_inputs": True,  # ä»»æ„ã®å…¥åŠ›ã‚’å—ã‘å…¥ã‚Œã‚‹
        },
    },
}


class DataCollectorSimulator(mosaik_api.Simulator):
    """
    ãƒ‡ãƒ¼ã‚¿åé›†å™¨ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚¿ãƒ¼ï¼ˆHILSç‰ˆï¼‰

    å…¨ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚¿ãƒ¼ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’åé›†ã—ã€HDF5ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ã€‚
    """

    def __init__(self):
        super().__init__(meta)
        self.entities = {}
        self.step_size = 1  # 1msæ¯ã«è¨˜éŒ²
        self.data_log = []

    def init(
        self,
        sid,
        time_resolution=0.001,
        step_size=1,
    ):
        """
        åˆæœŸåŒ–

        Args:
            sid: ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚¿ãƒ¼ID
            time_resolution: æ™‚é–“è§£åƒåº¦ï¼ˆ0.001 = 1msï¼‰
            step_size: ã‚¹ãƒ†ãƒƒãƒ—ã‚µã‚¤ã‚º
        """
        self.sid = sid
        self.time_resolution = time_resolution
        self.step_size = step_size
        return self.meta

    def create(self, num, model, output_dir=None):
        """
        ãƒ‡ãƒ¼ã‚¿ã‚³ãƒ¬ã‚¯ã‚¿ãƒ¼ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã®ä½œæˆ

        Args:
            num: ä½œæˆæ•°
            model: ãƒ¢ãƒ‡ãƒ«å
            output_dir: å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        """
        entities = []

        for i in range(num):
            eid = f"{model}_{i}"
            target_dir = Path(output_dir) if output_dir else Path.cwd()
            target_dir.mkdir(parents=True, exist_ok=True)

            self.entities[eid] = {
                "data": [],
                "current_time": 0,
                "output_dir": target_dir,
            }

            entities.append({"eid": eid, "type": model})
            print(f"[DataCollector] Created {eid} -> {target_dir}")

        return entities

    def step(self, time, inputs, max_advance=None):
        """
        ãƒ‡ãƒ¼ã‚¿åé›†ã‚¹ãƒ†ãƒƒãƒ—

        Args:
            time: ç¾åœ¨æ™‚åˆ» [ms]
            inputs: å…¥åŠ›ãƒ‡ãƒ¼ã‚¿
        """
        # å®Ÿæ™‚é–“ [ç§’]
        real_time = time * self.time_resolution

        for (
            eid,
            entity,
        ) in self.entities.items():
            entity["current_time"] = time

            if eid in inputs:
                # ãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ³ãƒˆã®ä½œæˆ
                data_point = {
                    "time_ms": time,  # ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³æ™‚åˆ» [ms]
                    "time_s": real_time,  # å®Ÿæ™‚é–“ [s]
                }

                # å…¨å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã‚’åé›†
                for attr, values in inputs[eid].items():
                    for (
                        source_eid,
                        value,
                    ) in values.items():
                        # å±æ€§åã¨ã‚½ãƒ¼ã‚¹IDã§ã‚­ãƒ¼ã‚’ä½œæˆ
                        key = f"{attr}_{source_eid}"

                        # å€¤ã®å‹ã«å¿œã˜ã¦å‡¦ç†
                        if isinstance(value, dict):
                            # è¾æ›¸å‹ï¼ˆä¾‹: commandï¼‰
                            # JSONæ–‡å­—åˆ—ã¨ã—ã¦ä¿å­˜
                            import json

                            data_point[key] = json.dumps(value)

                            # å„è¦ç´ ã‚‚å€‹åˆ¥ã«è¨˜éŒ²ï¼ˆãƒ—ãƒ­ãƒƒãƒˆç”¨ï¼‰
                            for (
                                k,
                                v,
                            ) in value.items():
                                data_point[f"{key}_{k}"] = v

                        elif isinstance(
                            value,
                            (int, float),
                        ):
                            # æ•°å€¤å‹
                            data_point[key] = value

                        elif value is None:
                            # Noneå€¤
                            data_point[key] = float("nan")

                        else:
                            # ãã®ä»–ï¼ˆæ–‡å­—åˆ—ç­‰ï¼‰
                            data_point[key] = str(value)

                entity["data"].append(data_point)
                self.data_log.append(data_point)

        return time + self.step_size

    def get_data(self, outputs):
        """
        ãƒ‡ãƒ¼ã‚¿å–å¾—ï¼ˆé€šå¸¸ã¯ä½¿ç”¨ã—ãªã„ï¼‰
        """
        return {}

    def finalize(self):
        """
        ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³çµ‚äº†æ™‚ã®å‡¦ç†

        åé›†ã—ãŸå…¨ãƒ‡ãƒ¼ã‚¿ã‚’HDF5ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ã€‚
        """
        if not self.entities:
            print("[DataCollector] No entities to finalize.")
            return

        if h5py is None:
            print(
                "[DataCollector] âš ï¸  h5py not available; skipped HDF5 export."
            )
            return

        if not self.data_log:
            print(
                "[DataCollector] No data collected; nothing to write."
            )
            return

        # å‡ºåŠ›å…ˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        first_entity = next(iter(self.entities.values()))
        output_dir: Path = first_entity["output_dir"]
        output_dir.mkdir(parents=True, exist_ok=True)

        output_path = output_dir / "hils_data.h5"

        print(
            f"\n[DataCollector] ğŸ’¾ Saving {len(self.data_log)} data points to HDF5..."
        )

        # å…¨ã‚­ãƒ¼ã‚’åé›†
        all_keys = sorted(
            {key for entry in self.data_log for key in entry.keys()}
        )

        with h5py.File(output_path, "w") as h5_file:
            # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
            h5_file.attrs["created_at"] = (
                datetime.utcnow().isoformat() + "Z"
            )
            h5_file.attrs["num_samples"] = len(self.data_log)
            h5_file.attrs["time_resolution"] = self.time_resolution

            # ãƒ‡ãƒ¼ã‚¿ã‚°ãƒ«ãƒ¼ãƒ—
            data_group = h5_file.create_group("data")

            # å„å±æ€§ã”ã¨ã«ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½œæˆ
            for key in all_keys:
                column = []
                for entry in self.data_log:
                    value = entry.get(key)

                    if value is None:
                        column.append(float("nan"))
                    elif isinstance(value, str):
                        # æ–‡å­—åˆ—ã¯ãã®ã¾ã¾
                        column.append(value)
                    elif isinstance(value, (int, float)):
                        column.append(float(value))
                    else:
                        column.append(str(value))

                # ãƒ‡ãƒ¼ã‚¿å‹ã«å¿œã˜ã¦ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½œæˆ
                if column and isinstance(column[0], str):
                    # æ–‡å­—åˆ—ãƒ‡ãƒ¼ã‚¿
                    data_group.create_dataset(
                        name=key,
                        data=column,
                        dtype=h5py.string_dtype(),
                    )
                else:
                    # æ•°å€¤ãƒ‡ãƒ¼ã‚¿
                    data_group.create_dataset(
                        name=key,
                        data=column,
                    )

            # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä¸€è¦§ã‚’è¡¨ç¤º
            print(f"[DataCollector] âœ… Saved datasets:")
            for key in sorted(data_group.keys()):
                dataset = data_group[key]
                print(f"  - {key}: {dataset.shape} {dataset.dtype}")

            # data_with_time_s ã‚°ãƒ«ãƒ¼ãƒ—ã‚’ä½œæˆï¼ˆtime_s ã‚’ xè»¸ã¨ã—ã¦å¯¾å¿œä»˜ã‘ï¼‰
            if "time_s" in data_group:
                print(f"\n[DataCollector] ğŸ“Š Creating data_with_time_s group...")
                time_s_data = data_group["time_s"][:]

                data_with_time = h5_file.create_group("data_with_time_s")
                data_with_time.attrs["description"] = "Datasets paired with time_s axis"
                data_with_time.attrs["source_group"] = "data"

                skip_keys = ["time_s", "time_ms"]
                created_count = 0

                for key in all_keys:
                    if key in skip_keys:
                        continue

                    # objectå‹ï¼ˆæ–‡å­—åˆ—ï¼‰ã¯ã‚¹ã‚­ãƒƒãƒ—
                    dataset = data_group[key]
                    try:
                        # æ•°å€¤å‹ã‹ãƒã‚§ãƒƒã‚¯
                        if not np.issubdtype(dataset.dtype, np.number):
                            continue
                    except TypeError:
                        continue

                    # 2æ¬¡å…ƒé…åˆ—ã¨ã—ã¦ä½œæˆ: (N, 2) where [:, 0]=time_s, [:, 1]=value
                    combined_data = np.column_stack((time_s_data, dataset[:]))

                    data_with_time.create_dataset(
                        name=key,
                        data=combined_data,
                        dtype=np.float64
                    )
                    data_with_time[key].attrs["columns"] = "time_s, value"
                    data_with_time[key].attrs["unit"] = key.split("_")[0]
                    created_count += 1

                print(f"[DataCollector] âœ… Created {created_count} datasets in data_with_time_s/ (Nx2 format)")

                # èª¬æ˜ã‚’æ›´æ–°
                data_with_time.attrs["description"] = "Datasets with time_s as x-axis (Nx2 arrays)"
                data_with_time.attrs["format"] = "Each dataset is (N, 2) where [:, 0]=time_s, [:, 1]=value"

        print(f"[DataCollector] ğŸ“ Output: {output_path}")


if __name__ == "__main__":
    mosaik_api.start_simulator(DataCollectorSimulator())
